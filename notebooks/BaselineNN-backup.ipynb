{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b8c86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "# import own functions\n",
    "from ipynb.fs.full.Functions import load_train_test_data, model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ccf1699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A snippet of the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>std</th>\n",
       "      <th>speaking_rate</th>\n",
       "      <th>articulation_rate</th>\n",
       "      <th>asd</th>\n",
       "      <th>...</th>\n",
       "      <th>lpcmfccs_local9_max</th>\n",
       "      <th>lpcmfccs_local9_min</th>\n",
       "      <th>lpcmfccs_local10_mean</th>\n",
       "      <th>lpcmfccs_local10_var</th>\n",
       "      <th>lpcmfccs_local10_max</th>\n",
       "      <th>lpcmfccs_local10_min</th>\n",
       "      <th>lpcmfccs_local11_mean</th>\n",
       "      <th>lpcmfccs_local11_var</th>\n",
       "      <th>lpcmfccs_local11_max</th>\n",
       "      <th>lpcmfccs_local11_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1.080531</td>\n",
       "      <td>0.798864</td>\n",
       "      <td>0.120244</td>\n",
       "      <td>-1.545379</td>\n",
       "      <td>-0.652108</td>\n",
       "      <td>0.523898</td>\n",
       "      <td>0.658707</td>\n",
       "      <td>-1.111672</td>\n",
       "      <td>-0.558179</td>\n",
       "      <td>0.331673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308694</td>\n",
       "      <td>0.084472</td>\n",
       "      <td>0.271426</td>\n",
       "      <td>-0.218007</td>\n",
       "      <td>-0.057589</td>\n",
       "      <td>0.313795</td>\n",
       "      <td>0.267861</td>\n",
       "      <td>-0.227202</td>\n",
       "      <td>-0.111190</td>\n",
       "      <td>0.307516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.720034</td>\n",
       "      <td>-1.009939</td>\n",
       "      <td>-0.702861</td>\n",
       "      <td>-0.141995</td>\n",
       "      <td>-0.651093</td>\n",
       "      <td>-0.969902</td>\n",
       "      <td>-1.111441</td>\n",
       "      <td>-0.398964</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>-0.177987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711811</td>\n",
       "      <td>-0.075413</td>\n",
       "      <td>-0.780710</td>\n",
       "      <td>-0.043641</td>\n",
       "      <td>0.130479</td>\n",
       "      <td>-0.665620</td>\n",
       "      <td>-0.891269</td>\n",
       "      <td>-0.010428</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>-0.722238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1.225277</td>\n",
       "      <td>-0.809615</td>\n",
       "      <td>-0.966255</td>\n",
       "      <td>0.733353</td>\n",
       "      <td>1.901097</td>\n",
       "      <td>-0.657665</td>\n",
       "      <td>-0.656852</td>\n",
       "      <td>-0.139340</td>\n",
       "      <td>0.054715</td>\n",
       "      <td>-0.207189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.255242</td>\n",
       "      <td>0.065525</td>\n",
       "      <td>-0.731827</td>\n",
       "      <td>0.102518</td>\n",
       "      <td>-0.035001</td>\n",
       "      <td>-1.208029</td>\n",
       "      <td>-0.794909</td>\n",
       "      <td>0.149507</td>\n",
       "      <td>-0.097226</td>\n",
       "      <td>-1.265853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.310436</td>\n",
       "      <td>-0.863672</td>\n",
       "      <td>-0.684753</td>\n",
       "      <td>0.296739</td>\n",
       "      <td>-0.651347</td>\n",
       "      <td>-0.873055</td>\n",
       "      <td>-0.962529</td>\n",
       "      <td>0.930037</td>\n",
       "      <td>0.782669</td>\n",
       "      <td>-0.712995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570754</td>\n",
       "      <td>-0.120425</td>\n",
       "      <td>0.484469</td>\n",
       "      <td>-0.230758</td>\n",
       "      <td>0.181230</td>\n",
       "      <td>0.541156</td>\n",
       "      <td>0.519439</td>\n",
       "      <td>-0.246612</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>0.577385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-0.541554</td>\n",
       "      <td>1.990422</td>\n",
       "      <td>2.001863</td>\n",
       "      <td>-1.172855</td>\n",
       "      <td>-0.651601</td>\n",
       "      <td>1.882264</td>\n",
       "      <td>1.798970</td>\n",
       "      <td>1.191835</td>\n",
       "      <td>1.071873</td>\n",
       "      <td>-0.883360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.591040</td>\n",
       "      <td>0.194870</td>\n",
       "      <td>0.515836</td>\n",
       "      <td>-0.230930</td>\n",
       "      <td>-0.206997</td>\n",
       "      <td>0.556228</td>\n",
       "      <td>0.565981</td>\n",
       "      <td>-0.247006</td>\n",
       "      <td>-0.185268</td>\n",
       "      <td>0.598848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     duration      mean    median       max       min       var       std  \\\n",
       "241  1.080531  0.798864  0.120244 -1.545379 -0.652108  0.523898  0.658707   \n",
       "364  0.720034 -1.009939 -0.702861 -0.141995 -0.651093 -0.969902 -1.111441   \n",
       "209  1.225277 -0.809615 -0.966255  0.733353  1.901097 -0.657665 -0.656852   \n",
       "430  0.310436 -0.863672 -0.684753  0.296739 -0.651347 -0.873055 -0.962529   \n",
       "309 -0.541554  1.990422  2.001863 -1.172855 -0.651601  1.882264  1.798970   \n",
       "\n",
       "     speaking_rate  articulation_rate       asd  ...  lpcmfccs_local9_max  \\\n",
       "241      -1.111672          -0.558179  0.331673  ...            -0.308694   \n",
       "364      -0.398964           0.017716 -0.177987  ...             0.711811   \n",
       "209      -0.139340           0.054715 -0.207189  ...             1.255242   \n",
       "430       0.930037           0.782669 -0.712995  ...            -0.570754   \n",
       "309       1.191835           1.071873 -0.883360  ...            -0.591040   \n",
       "\n",
       "     lpcmfccs_local9_min  lpcmfccs_local10_mean  lpcmfccs_local10_var  \\\n",
       "241             0.084472               0.271426             -0.218007   \n",
       "364            -0.075413              -0.780710             -0.043641   \n",
       "209             0.065525              -0.731827              0.102518   \n",
       "430            -0.120425               0.484469             -0.230758   \n",
       "309             0.194870               0.515836             -0.230930   \n",
       "\n",
       "     lpcmfccs_local10_max  lpcmfccs_local10_min  lpcmfccs_local11_mean  \\\n",
       "241             -0.057589              0.313795               0.267861   \n",
       "364              0.130479             -0.665620              -0.891269   \n",
       "209             -0.035001             -1.208029              -0.794909   \n",
       "430              0.181230              0.541156               0.519439   \n",
       "309             -0.206997              0.556228               0.565981   \n",
       "\n",
       "     lpcmfccs_local11_var  lpcmfccs_local11_max  lpcmfccs_local11_min  \n",
       "241             -0.227202             -0.111190              0.307516  \n",
       "364             -0.010428              0.017714             -0.722238  \n",
       "209              0.149507             -0.097226             -1.265853  \n",
       "430             -0.246612              0.057740              0.577385  \n",
       "309             -0.247006             -0.185268              0.598848  \n",
       "\n",
       "[5 rows x 249 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 454 entries with 249 columns in the training data.\n",
      "There are 81 entries with 249 columns in the testing data.\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df = pd.read_pickle('../results/df_prep_numerical_only.pkl')\n",
    "\n",
    "# load the train/test data\n",
    "X_train, X_test, y_train, y_test = load_train_test_data(df)\n",
    "num_classes = y_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1ed40d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A snippet of the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>std</th>\n",
       "      <th>speaking_rate</th>\n",
       "      <th>articulation_rate</th>\n",
       "      <th>asd</th>\n",
       "      <th>...</th>\n",
       "      <th>lpcmfccs_local9_max</th>\n",
       "      <th>lpcmfccs_local9_min</th>\n",
       "      <th>lpcmfccs_local10_mean</th>\n",
       "      <th>lpcmfccs_local10_var</th>\n",
       "      <th>lpcmfccs_local10_max</th>\n",
       "      <th>lpcmfccs_local10_min</th>\n",
       "      <th>lpcmfccs_local11_mean</th>\n",
       "      <th>lpcmfccs_local11_var</th>\n",
       "      <th>lpcmfccs_local11_max</th>\n",
       "      <th>lpcmfccs_local11_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.467142</td>\n",
       "      <td>1.539161</td>\n",
       "      <td>-0.322586</td>\n",
       "      <td>0.530244</td>\n",
       "      <td>-0.652108</td>\n",
       "      <td>2.430749</td>\n",
       "      <td>2.197872</td>\n",
       "      <td>-1.446222</td>\n",
       "      <td>-0.420712</td>\n",
       "      <td>0.199741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.450101</td>\n",
       "      <td>0.157607</td>\n",
       "      <td>0.426546</td>\n",
       "      <td>-0.228943</td>\n",
       "      <td>-0.150820</td>\n",
       "      <td>0.440161</td>\n",
       "      <td>0.448570</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>-0.167025</td>\n",
       "      <td>0.452311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-0.985224</td>\n",
       "      <td>0.249976</td>\n",
       "      <td>0.339190</td>\n",
       "      <td>-0.054813</td>\n",
       "      <td>-0.652108</td>\n",
       "      <td>-0.041486</td>\n",
       "      <td>0.085751</td>\n",
       "      <td>1.483441</td>\n",
       "      <td>1.394006</td>\n",
       "      <td>-1.056562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669442</td>\n",
       "      <td>0.198621</td>\n",
       "      <td>-1.832046</td>\n",
       "      <td>0.366989</td>\n",
       "      <td>-0.207596</td>\n",
       "      <td>-0.623751</td>\n",
       "      <td>-1.939841</td>\n",
       "      <td>0.469775</td>\n",
       "      <td>-0.190018</td>\n",
       "      <td>-0.679760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-0.678938</td>\n",
       "      <td>0.829002</td>\n",
       "      <td>-0.301186</td>\n",
       "      <td>-1.202544</td>\n",
       "      <td>-0.651347</td>\n",
       "      <td>1.323608</td>\n",
       "      <td>1.360837</td>\n",
       "      <td>-1.089735</td>\n",
       "      <td>-0.503961</td>\n",
       "      <td>0.278790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108350</td>\n",
       "      <td>0.200731</td>\n",
       "      <td>0.283543</td>\n",
       "      <td>-0.208727</td>\n",
       "      <td>-0.207894</td>\n",
       "      <td>0.128361</td>\n",
       "      <td>0.288420</td>\n",
       "      <td>-0.216085</td>\n",
       "      <td>-0.205875</td>\n",
       "      <td>0.103808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-1.281349</td>\n",
       "      <td>-0.185806</td>\n",
       "      <td>-0.255092</td>\n",
       "      <td>-1.059048</td>\n",
       "      <td>-0.651601</td>\n",
       "      <td>-0.143899</td>\n",
       "      <td>-0.027428</td>\n",
       "      <td>-0.262226</td>\n",
       "      <td>-0.534406</td>\n",
       "      <td>0.308346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592627</td>\n",
       "      <td>0.195681</td>\n",
       "      <td>0.520823</td>\n",
       "      <td>-0.230936</td>\n",
       "      <td>-0.207496</td>\n",
       "      <td>0.557323</td>\n",
       "      <td>0.574294</td>\n",
       "      <td>-0.247029</td>\n",
       "      <td>-0.187369</td>\n",
       "      <td>0.600547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.303987</td>\n",
       "      <td>2.509715</td>\n",
       "      <td>2.604376</td>\n",
       "      <td>0.672797</td>\n",
       "      <td>-0.652108</td>\n",
       "      <td>2.694004</td>\n",
       "      <td>2.380136</td>\n",
       "      <td>-0.145141</td>\n",
       "      <td>-0.405064</td>\n",
       "      <td>0.185164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596886</td>\n",
       "      <td>0.178225</td>\n",
       "      <td>0.523121</td>\n",
       "      <td>-0.230939</td>\n",
       "      <td>-0.179220</td>\n",
       "      <td>0.560123</td>\n",
       "      <td>0.578755</td>\n",
       "      <td>-0.247037</td>\n",
       "      <td>-0.178977</td>\n",
       "      <td>0.605140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     duration      mean    median       max       min       var       std  \\\n",
       "50  -0.467142  1.539161 -0.322586  0.530244 -0.652108  2.430749  2.197872   \n",
       "117 -0.985224  0.249976  0.339190 -0.054813 -0.652108 -0.041486  0.085751   \n",
       "190 -0.678938  0.829002 -0.301186 -1.202544 -0.651347  1.323608  1.360837   \n",
       "285 -1.281349 -0.185806 -0.255092 -1.059048 -0.651601 -0.143899 -0.027428   \n",
       "310  0.303987  2.509715  2.604376  0.672797 -0.652108  2.694004  2.380136   \n",
       "\n",
       "     speaking_rate  articulation_rate       asd  ...  lpcmfccs_local9_max  \\\n",
       "50       -1.446222          -0.420712  0.199741  ...            -0.450101   \n",
       "117       1.483441           1.394006 -1.056562  ...             0.669442   \n",
       "190      -1.089735          -0.503961  0.278790  ...            -0.108350   \n",
       "285      -0.262226          -0.534406  0.308346  ...            -0.592627   \n",
       "310      -0.145141          -0.405064  0.185164  ...            -0.596886   \n",
       "\n",
       "     lpcmfccs_local9_min  lpcmfccs_local10_mean  lpcmfccs_local10_var  \\\n",
       "50              0.157607               0.426546             -0.228943   \n",
       "117             0.198621              -1.832046              0.366989   \n",
       "190             0.200731               0.283543             -0.208727   \n",
       "285             0.195681               0.520823             -0.230936   \n",
       "310             0.178225               0.523121             -0.230939   \n",
       "\n",
       "     lpcmfccs_local10_max  lpcmfccs_local10_min  lpcmfccs_local11_mean  \\\n",
       "50              -0.150820              0.440161               0.448570   \n",
       "117             -0.207596             -0.623751              -1.939841   \n",
       "190             -0.207894              0.128361               0.288420   \n",
       "285             -0.207496              0.557323               0.574294   \n",
       "310             -0.179220              0.560123               0.578755   \n",
       "\n",
       "     lpcmfccs_local11_var  lpcmfccs_local11_max  lpcmfccs_local11_min  \n",
       "50              -0.243507             -0.167025              0.452311  \n",
       "117              0.469775             -0.190018             -0.679760  \n",
       "190             -0.216085             -0.205875              0.103808  \n",
       "285             -0.247029             -0.187369              0.600547  \n",
       "310             -0.247037             -0.178977              0.605140  \n",
       "\n",
       "[5 rows x 249 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 entries with 249 columns in the validation data.\n",
      "There are 41 entries with 249 columns in the testing data.\n"
     ]
    }
   ],
   "source": [
    "# split test set into validation and test data\n",
    "df_test = pd.concat([X_test, y_test], axis = 1)\n",
    "X_val, X_test, y_val, y_test = load_train_test_data(df_test, test_size=0.5, split_type='val/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48421206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tabluar dataset with torch\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data.iloc[idx].values, dtype=torch.float)\n",
    "        y = torch.tensor(self.target.iloc[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "48553f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device state: cpu\n"
     ]
    }
   ],
   "source": [
    "# GPU device\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device state:', device)\n",
    "\n",
    "# set hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 64\n",
    "EPOCHS = 25\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2879605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train, test, and val loader\n",
    "torch.manual_seed(0) # set random seed\n",
    "train_loader = DataLoader(TabularDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TabularDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(TabularDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbb0f0",
   "metadata": {},
   "source": [
    "# MLP-FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8213d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cb8361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.stack([torch.Tensor(f) for f in x])  # Convert input to tensor if not already\n",
    "        x = x.view(x.size(0), -1)  # Flatten input if necessary\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "980c745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, optimizer, and criterion\n",
    "model = MLP(input_size=len(X_train.columns), hidden_size=HIDDEN_SIZE, output_size=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da8ab313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, batch_size=BATCH_SIZE, num_epochs=EPOCHS, patience=patience):\n",
    "    # set random seed\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1_scores = []\n",
    "    best_val_f1_score = 0.0\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_preds.extend(predicted.tolist())\n",
    "                val_labels.extend(labels.tolist())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        val_f1_scores.append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 Macro: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1_score:\n",
    "            best_val_f1_score = val_f1\n",
    "            best_model_weights = model.state_dict()\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count == patience:\n",
    "                print(f\"No improvement in val F1 macro score for {patience} epochs, stopping training early\")\n",
    "                break\n",
    "        \n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, train_losses, val_losses, val_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b54941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(X, y, hidden_sizes, batch_size=64, learning_rate=0.01, num_epochs=100, \n",
    "              validation_split=0.2, patience=5, metric='accuracy', random_state=None):\n",
    "    \"\"\"\n",
    "    Train a multilayer perceptron (MLP) for classification.\n",
    "\n",
    "    Args:\n",
    "        X (list or numpy.ndarray): Input features. Can be a list of arrays or a 2D array.\n",
    "        y (list or numpy.ndarray): Target labels.\n",
    "        hidden_sizes (list): Sizes of the hidden layers.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 64.\n",
    "        learning_rate (float, optional): Learning rate for training. Defaults to 0.01.\n",
    "        num_epochs (int, optional): Number of epochs to train for. Defaults to 100.\n",
    "        validation_split (float, optional): Fraction of the data to use for validation. Defaults to 0.2.\n",
    "        patience (int, optional): Number of epochs to wait before early stopping if validation loss does not improve. Defaults to 5.\n",
    "        metric (str, optional): Metric to use for early stopping. Can be 'accuracy' or 'f1_macro'. Defaults to 'accuracy'.\n",
    "        random_state (int or None, optional): Random state for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the trained MLP and the history of training and validation metrics.\n",
    "    \"\"\"\n",
    "    # Determine the input feature size\n",
    "    if isinstance(X, list):\n",
    "        feature_lengths = [len(x) for x in X]\n",
    "        input_feature_size = sum(feature_lengths)\n",
    "        X = [pad_vector(x, max(feature_lengths)) for x in X]\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "    else:\n",
    "        input_feature_size = X.shape[1]\n",
    "\n",
    "    # Convert the target labels to a tensor\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, random_state=random_state)\n",
    "\n",
    "    # Create data loaders for the training and validation sets\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e5965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 1.8222, Val Loss: 1.6959, Val F1 Macro: 0.3294\n",
      "Epoch 2/25, Train Loss: 1.4374, Val Loss: 1.5506, Val F1 Macro: 0.4514\n",
      "Epoch 3/25, Train Loss: 1.1338, Val Loss: 1.2720, Val F1 Macro: 0.4928\n",
      "Epoch 4/25, Train Loss: 0.9228, Val Loss: 0.9188, Val F1 Macro: 0.5450\n",
      "Epoch 5/25, Train Loss: 0.7393, Val Loss: 0.9221, Val F1 Macro: 0.6277\n",
      "Epoch 6/25, Train Loss: 0.6127, Val Loss: 0.6969, Val F1 Macro: 0.6320\n",
      "Epoch 7/25, Train Loss: 0.4600, Val Loss: 0.6905, Val F1 Macro: 0.6696\n",
      "Epoch 8/25, Train Loss: 0.3719, Val Loss: 0.7409, Val F1 Macro: 0.7575\n",
      "Epoch 9/25, Train Loss: 0.3181, Val Loss: 0.8802, Val F1 Macro: 0.7985\n",
      "Epoch 10/25, Train Loss: 0.2523, Val Loss: 0.6630, Val F1 Macro: 0.8397\n",
      "Epoch 11/25, Train Loss: 0.2276, Val Loss: 0.6531, Val F1 Macro: 0.8473\n",
      "Epoch 12/25, Train Loss: 0.1719, Val Loss: 0.7929, Val F1 Macro: 0.8515\n",
      "Epoch 13/25, Train Loss: 0.1406, Val Loss: 0.4136, Val F1 Macro: 0.8384\n",
      "Epoch 14/25, Train Loss: 0.1217, Val Loss: 0.7844, Val F1 Macro: 0.8975\n",
      "Epoch 15/25, Train Loss: 0.0902, Val Loss: 0.3842, Val F1 Macro: 0.8663\n",
      "Epoch 16/25, Train Loss: 0.0723, Val Loss: 0.3534, Val F1 Macro: 0.8650\n",
      "No improvement in val F1 macro score for 2 epochs, stopping training early\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses, val_f1_scores = train_model(model, criterion, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be48e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: f1_score\n",
      "99.57% for the train data\n",
      "81.58% for the test data\n",
      "\n",
      "Evaluation: accuracy_score\n",
      "99.56% for the train data\n",
      "85.37% for the test data\n",
      "\n",
      "Evaluation: precision_score\n",
      "99.63% for the train data\n",
      "89.90% for the test data\n",
      "\n",
      "Evaluation: recall_score\n",
      "99.52% for the train data\n",
      "80.95% for the test data\n",
      "\n",
      "Evaluation: confusion_matrix of test predictions\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hard cases for the model:\n",
      "\n",
      "Label: disgust(2)\n",
      "Hard cases of false negatives: ['anger(0)', 'sadness(6)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_eval(model, X_train, X_test, y_train, y_test, confusion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4478434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "pickle.dump(model, open('../results/models/MLPFFNN_best_model_all_numerical_features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c37f69",
   "metadata": {},
   "source": [
    "# GFFNN (skip connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fc4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithSkip(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPWithSkip, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.skip_connection = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden1 = self.relu(self.fc1(x))\n",
    "        hidden2 = self.relu(self.fc2(hidden1))\n",
    "        \n",
    "        # Apply skip connection from input to second hidden layer\n",
    "        skip = self.skip_connection(x)\n",
    "        hidden2_skip = hidden2 + skip\n",
    "        \n",
    "        output = self.fc3(hidden2_skip)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368fdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, optimizer, and criterion\n",
    "model = MLPWithSkip(input_size=len(X_train.columns), hidden_size=HIDDEN_SIZE, output_size=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d5ef657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 1.4954, Val Loss: 1.2778, Val F1 Macro: 0.4518\n",
      "Epoch 2/25, Train Loss: 0.9119, Val Loss: 1.1691, Val F1 Macro: 0.5547\n",
      "Epoch 3/25, Train Loss: 0.6864, Val Loss: 0.9944, Val F1 Macro: 0.5643\n",
      "Epoch 4/25, Train Loss: 0.5492, Val Loss: 0.6039, Val F1 Macro: 0.6753\n",
      "Epoch 5/25, Train Loss: 0.4348, Val Loss: 0.7281, Val F1 Macro: 0.6554\n",
      "Epoch 6/25, Train Loss: 0.3897, Val Loss: 0.6064, Val F1 Macro: 0.7167\n",
      "Epoch 7/25, Train Loss: 0.2927, Val Loss: 0.5481, Val F1 Macro: 0.8190\n",
      "Epoch 8/25, Train Loss: 0.2425, Val Loss: 0.5449, Val F1 Macro: 0.8190\n",
      "Epoch 9/25, Train Loss: 0.2079, Val Loss: 0.7370, Val F1 Macro: 0.7951\n",
      "No improvement in val F1 macro score for 2 epochs, stopping training early\n"
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses, val_f1_scores = train_model(model, criterion, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5e4469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: f1_score\n",
      "96.50% for the train data\n",
      "83.35% for the test data\n",
      "\n",
      "Evaluation: accuracy_score\n",
      "96.48% for the train data\n",
      "87.80% for the test data\n",
      "\n",
      "Evaluation: precision_score\n",
      "96.78% for the train data\n",
      "90.48% for the test data\n",
      "\n",
      "Evaluation: recall_score\n",
      "96.33% for the train data\n",
      "82.14% for the test data\n",
      "\n",
      "Evaluation: confusion_matrix of test predictions\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hard cases for the model:\n",
      "\n",
      "Label: disgust(2)\n",
      "Hard cases of false negatives: ['anger(0)', 'sadness(6)']\n",
      "\n",
      "Label: sadness(6)\n",
      "Hard cases of false negatives: ['neutral(5)']\n",
      "Hard cases of false positives: ['disgust(2)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_eval(model, X_train, X_test, y_train, y_test, confusion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ecd26",
   "metadata": {},
   "source": [
    "# Test for features paper 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "26fec18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe with all entries (not only numerical) - note that they are not scaled / further preprocessed\n",
    "df = pd.read_pickle('../results/df_prep_numerical_only.pkl')\n",
    "df_all = pd.read_pickle('../results/extracted_features_modified.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a0e1add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of features (mean pitch, mean energy, pitch variance, skew of logarithmic pitch, range of logarithmic pitch,\n",
    "# range of logarithmic energy)\n",
    "df_paper1 = df_all[['lpc', 'log_energy_entropy', 'shannon_entropy', 'threshold_entropy',\n",
    "                   'sure_entropy', 'rms', 'f0', 'f1', 'f2', 'f3', 'f4']].copy()\n",
    "df_paper1 = pd.merge(df[['label', 'var', 'min', 'median']], df_paper1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b9667869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>var</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>lpc</th>\n",
       "      <th>log_energy_entropy</th>\n",
       "      <th>shannon_entropy</th>\n",
       "      <th>threshold_entropy</th>\n",
       "      <th>sure_entropy</th>\n",
       "      <th>rms</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[-1.1077997442702396]</td>\n",
       "      <td>[0.9120471048692147]</td>\n",
       "      <td>[-1.0600887065143705]</td>\n",
       "      <td>[1.0, -1.3784221, 0.69826585, -0.44159302, 0.2...</td>\n",
       "      <td>[0.8509675, 0.37695062, 0.0, 0.2376355, 0.5616...</td>\n",
       "      <td>[0.0, 0.24065953946723917, 0.3056793132108977,...</td>\n",
       "      <td>[0.0, 0.0, 0.025390625, 0.1650390625, 0.390625...</td>\n",
       "      <td>[0.0, 0.5340640447023426, 1.0, 0.7605845690952...</td>\n",
       "      <td>[0.0011372825, 0.0012941798, 0.012547005, 0.07...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 141.8...</td>\n",
       "      <td>[366.139456692797, 406.2651059763895, 464.1082...</td>\n",
       "      <td>[1888.5991203215656, 1888.0465037258002, 1912....</td>\n",
       "      <td>[2622.348075806301, 2640.683019940289, 2646.09...</td>\n",
       "      <td>[3650.5303260741794, 3616.192663764363, 3618.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[-0.18546201004620894]</td>\n",
       "      <td>[-0.6510932114283963]</td>\n",
       "      <td>[-0.9893016606113726]</td>\n",
       "      <td>[1.0, -1.5548326, 0.84461135, -0.39231607, 0.1...</td>\n",
       "      <td>[0.77799094, 0.3398105, 0.007011771, 0.2680765...</td>\n",
       "      <td>[0.0, 0.29996965968948575, 0.4304300402658555,...</td>\n",
       "      <td>[0.0, 0.0, 0.00830078125, 0.1806640625, 0.3408...</td>\n",
       "      <td>[0.0, 0.54618213281419, 1.0, 0.942677914773956...</td>\n",
       "      <td>[0.0023372034, 0.002490107, 0.0158029, 0.09679...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[455.1342622238156, 474.13099161767065, 511.05...</td>\n",
       "      <td>[1710.5641941221943, 1677.8259451018241, 1679....</td>\n",
       "      <td>[2032.8323008676925, 2660.126131933908, 2546.2...</td>\n",
       "      <td>[2662.473597581669, 3074.6056154602775, 3273.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.13085934083665982]</td>\n",
       "      <td>[-0.6521084024115413]</td>\n",
       "      <td>[-0.6765216903423126]</td>\n",
       "      <td>[1.0, -1.366485, 0.840961, -0.37688726, 0.1708...</td>\n",
       "      <td>[0.6902827, 0.34663463, 0.0, 0.18346202, 0.434...</td>\n",
       "      <td>[0.0, 0.14975822138778494, 0.2186258363367165,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0908203125, 0.271484375, 0.4...</td>\n",
       "      <td>[0.11014711429649193, 0.5811014711429651, 0.88...</td>\n",
       "      <td>[0.0003959035, 0.0005016159, 0.000568984, 0.04...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 155.56349186104043, ...</td>\n",
       "      <td>[412.2984102092574, 457.16085329536537, 494.73...</td>\n",
       "      <td>[1910.583625157631, 1912.489911862922, 1902.88...</td>\n",
       "      <td>[2687.4015293731413, 2687.012378039779, 2689.1...</td>\n",
       "      <td>[3211.25186744456, 3124.9491229330333, 3162.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.20664944494006618]</td>\n",
       "      <td>[-0.024212779336319757]</td>\n",
       "      <td>[-0.5217779155776198]</td>\n",
       "      <td>[1.0, -1.1566716, 0.09517725, 0.10888941, 0.07...</td>\n",
       "      <td>[0.7458475, 0.3505782, 0.21594667, 0.49595094,...</td>\n",
       "      <td>[0.0, 0.24714613308195366, 0.24408760946910923...</td>\n",
       "      <td>[0.0, 0.0, 0.15966796875, 0.369140625, 0.46875...</td>\n",
       "      <td>[0.0, 0.5325571366968522, 0.5741699008193186, ...</td>\n",
       "      <td>[0.0020485923, 0.004919004, 0.10309043, 0.1909...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 223.84553226259098, 235.7...</td>\n",
       "      <td>[467.48298270792884, 528.4268420078914, 598.64...</td>\n",
       "      <td>[1645.6773404405674, 1663.4480906796925, 1682....</td>\n",
       "      <td>[2508.0246943872235, 2576.851081680795, 2634.9...</td>\n",
       "      <td>[3766.8507541790223, 3893.700444236606, 3862.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[0.6880066826870322]</td>\n",
       "      <td>[-0.6510932114283963]</td>\n",
       "      <td>[-0.4789764459618536]</td>\n",
       "      <td>[1.0, -1.4480726, 0.6665786, -0.40154517, 0.24...</td>\n",
       "      <td>[0.7931449, 0.45727265, 0.16271043, 0.36254406...</td>\n",
       "      <td>[0.0, 0.366053164305711, 0.6473166578900265, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0009765625, 0.1318359375, 0.17822...</td>\n",
       "      <td>[0.0, 0.5514785236347939, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.0043478855, 0.005019508, 0.011774572, 0.065...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 143.4...</td>\n",
       "      <td>[422.45062471781057, 420.65528076393826, 407.2...</td>\n",
       "      <td>[1533.57873113302, 1536.35804946221, 1530.8294...</td>\n",
       "      <td>[2599.126752660879, 2500.3316066946827, 2557.4...</td>\n",
       "      <td>[3834.8613551078865, 3813.5172669026283, 3767....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                     var                      min  \\\n",
       "0      4   [-1.1077997442702396]     [0.9120471048692147]   \n",
       "1      5  [-0.18546201004620894]    [-0.6510932114283963]   \n",
       "2      0  [-0.13085934083665982]    [-0.6521084024115413]   \n",
       "3      4  [-0.20664944494006618]  [-0.024212779336319757]   \n",
       "4      5    [0.6880066826870322]    [-0.6510932114283963]   \n",
       "\n",
       "                  median                                                lpc  \\\n",
       "0  [-1.0600887065143705]  [1.0, -1.3784221, 0.69826585, -0.44159302, 0.2...   \n",
       "1  [-0.9893016606113726]  [1.0, -1.5548326, 0.84461135, -0.39231607, 0.1...   \n",
       "2  [-0.6765216903423126]  [1.0, -1.366485, 0.840961, -0.37688726, 0.1708...   \n",
       "3  [-0.5217779155776198]  [1.0, -1.1566716, 0.09517725, 0.10888941, 0.07...   \n",
       "4  [-0.4789764459618536]  [1.0, -1.4480726, 0.6665786, -0.40154517, 0.24...   \n",
       "\n",
       "                                  log_energy_entropy  \\\n",
       "0  [0.8509675, 0.37695062, 0.0, 0.2376355, 0.5616...   \n",
       "1  [0.77799094, 0.3398105, 0.007011771, 0.2680765...   \n",
       "2  [0.6902827, 0.34663463, 0.0, 0.18346202, 0.434...   \n",
       "3  [0.7458475, 0.3505782, 0.21594667, 0.49595094,...   \n",
       "4  [0.7931449, 0.45727265, 0.16271043, 0.36254406...   \n",
       "\n",
       "                                     shannon_entropy  \\\n",
       "0  [0.0, 0.24065953946723917, 0.3056793132108977,...   \n",
       "1  [0.0, 0.29996965968948575, 0.4304300402658555,...   \n",
       "2  [0.0, 0.14975822138778494, 0.2186258363367165,...   \n",
       "3  [0.0, 0.24714613308195366, 0.24408760946910923...   \n",
       "4  [0.0, 0.366053164305711, 0.6473166578900265, 0...   \n",
       "\n",
       "                                   threshold_entropy  \\\n",
       "0  [0.0, 0.0, 0.025390625, 0.1650390625, 0.390625...   \n",
       "1  [0.0, 0.0, 0.00830078125, 0.1806640625, 0.3408...   \n",
       "2  [0.0, 0.0, 0.0, 0.0908203125, 0.271484375, 0.4...   \n",
       "3  [0.0, 0.0, 0.15966796875, 0.369140625, 0.46875...   \n",
       "4  [0.0, 0.0, 0.0009765625, 0.1318359375, 0.17822...   \n",
       "\n",
       "                                        sure_entropy  \\\n",
       "0  [0.0, 0.5340640447023426, 1.0, 0.7605845690952...   \n",
       "1  [0.0, 0.54618213281419, 1.0, 0.942677914773956...   \n",
       "2  [0.11014711429649193, 0.5811014711429651, 0.88...   \n",
       "3  [0.0, 0.5325571366968522, 0.5741699008193186, ...   \n",
       "4  [0.0, 0.5514785236347939, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "\n",
       "                                                 rms  \\\n",
       "0  [0.0011372825, 0.0012941798, 0.012547005, 0.07...   \n",
       "1  [0.0023372034, 0.002490107, 0.0158029, 0.09679...   \n",
       "2  [0.0003959035, 0.0005016159, 0.000568984, 0.04...   \n",
       "3  [0.0020485923, 0.004919004, 0.10309043, 0.1909...   \n",
       "4  [0.0043478855, 0.005019508, 0.011774572, 0.065...   \n",
       "\n",
       "                                                  f0  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 141.8...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 155.56349186104043, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 223.84553226259098, 235.7...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 143.4...   \n",
       "\n",
       "                                                  f1  \\\n",
       "0  [366.139456692797, 406.2651059763895, 464.1082...   \n",
       "1  [455.1342622238156, 474.13099161767065, 511.05...   \n",
       "2  [412.2984102092574, 457.16085329536537, 494.73...   \n",
       "3  [467.48298270792884, 528.4268420078914, 598.64...   \n",
       "4  [422.45062471781057, 420.65528076393826, 407.2...   \n",
       "\n",
       "                                                  f2  \\\n",
       "0  [1888.5991203215656, 1888.0465037258002, 1912....   \n",
       "1  [1710.5641941221943, 1677.8259451018241, 1679....   \n",
       "2  [1910.583625157631, 1912.489911862922, 1902.88...   \n",
       "3  [1645.6773404405674, 1663.4480906796925, 1682....   \n",
       "4  [1533.57873113302, 1536.35804946221, 1530.8294...   \n",
       "\n",
       "                                                  f3  \\\n",
       "0  [2622.348075806301, 2640.683019940289, 2646.09...   \n",
       "1  [2032.8323008676925, 2660.126131933908, 2546.2...   \n",
       "2  [2687.4015293731413, 2687.012378039779, 2689.1...   \n",
       "3  [2508.0246943872235, 2576.851081680795, 2634.9...   \n",
       "4  [2599.126752660879, 2500.3316066946827, 2557.4...   \n",
       "\n",
       "                                                  f4  \n",
       "0  [3650.5303260741794, 3616.192663764363, 3618.7...  \n",
       "1  [2662.473597581669, 3074.6056154602775, 3273.4...  \n",
       "2  [3211.25186744456, 3124.9491229330333, 3162.11...  \n",
       "3  [3766.8507541790223, 3893.700444236606, 3862.1...  \n",
       "4  [3834.8613551078865, 3813.5172669026283, 3767....  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small preprocess: convert var, min, median also to arrays\n",
    "for feature in ['var', 'min', 'median']:\n",
    "    df_paper1[feature] = df_paper1[feature].apply(lambda x: np.array([x]))\n",
    "df_paper1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7f1a0eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 15)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paper1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "15869e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lpc: [5]\n",
      "log_energy_entropy: [ 39  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133\n",
      " 137 140 141 142 146 154 156 158 159 161 162 163 166 167 168 170 172 174\n",
      " 176 178 185 187 188 195 211 213 240 281]\n",
      "shannon_entropy: [ 39  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133\n",
      " 137 140 141 142 146 154 156 158 159 161 162 163 166 167 168 170 172 174\n",
      " 176 178 185 187 188 195 211 213 240 281]\n",
      "threshold_entropy: [ 39  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133\n",
      " 137 140 141 142 146 154 156 158 159 161 162 163 166 167 168 170 172 174\n",
      " 176 178 185 187 188 195 211 213 240 281]\n",
      "sure_entropy: [ 39  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133\n",
      " 137 140 141 142 146 154 156 158 159 161 162 163 166 167 168 170 172 174\n",
      " 176 178 185 187 188 195 211 213 240 281]\n",
      "rms: [ 39  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133\n",
      " 137 140 141 142 146 154 156 158 159 161 162 163 166 167 168 170 172 174\n",
      " 176 178 185 187 188 195 211 213 240 281]\n",
      "f0: [ 39  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133\n",
      " 137 140 141 142 146 154 156 158 159 161 162 163 166 167 168 170 172 174\n",
      " 176 178 185 187 188 195 211 213 240 281]\n",
      "f1: [ 64  71  77  83  86  88  94  95  97  99 101 104 107 110 112 113 114 115\n",
      " 117 118 121 123 126 127 128 130 131 133 136 137 140 142 143 145 146 147\n",
      " 150 151 152 154 157 159 160 161 162 163 165 166 167 169 171 172 173 174\n",
      " 175 176 177 178 179 180 186 187 188 189 190 192 193 194 195 196 197 199\n",
      " 200 201 202 203 205 206 209 210 211 214 216 217 219 220 222 223 224 225\n",
      " 226 227 229 230 232 233 234 236 237 238 239 240 241 243 244 245 247 250\n",
      " 251 252 253 254 255 256 257 258 260 261 263 264 265 267 268 269 270 272\n",
      " 273 274 276 277 278 279 280 281 282 284 285 286 287 288 289 291 292 293\n",
      " 294 295 296 297 298 299 300 301 302 304 307 310 311 312 313 314 316 318\n",
      " 319 323 324 325 326 327 328 329 331 332 334 335 337 338 340 341 343 344\n",
      " 345 347 349 350 351 353 354 357 360 365 366 368 370 371 372 374 378 380\n",
      " 381 382 383 384 386 388 390 391 392 393 394 395 396 397 399 401 403 405\n",
      " 406 408 409 411 412 414 415 422 423 424 427 428 429 430 431 432 434 435\n",
      " 436 438 439 440 444 445 446 447 448 451 452 453 454 455 462 463 464 468\n",
      " 470 471 473 474 476 478 479 480 483 484 489 490 493 494 501 502 503 505\n",
      " 508 510 511 513 515 523 524 526 527 530 536 537 540 544 548 551 552 554\n",
      " 560 563 566 570 575 583 585 586 589 590 591 593 596 600 607 608 611 615\n",
      " 623 629 636 637 645 648 659 661 672 677 678 685 701 703 713 727 740 742\n",
      " 745 746 747 757 766 769 771 779 814 825 844 889]\n",
      "f2: [ 64  71  77  83  86  88  94  95  97  99 101 104 107 110 112 113 114 115\n",
      " 117 118 121 123 126 127 128 130 131 133 136 137 140 142 143 145 146 147\n",
      " 150 151 152 154 157 159 160 161 162 163 165 166 167 169 171 172 173 174\n",
      " 175 176 177 178 179 180 186 187 188 189 190 192 193 194 195 196 197 199\n",
      " 200 201 202 203 205 206 209 210 211 214 216 217 219 220 222 223 224 225\n",
      " 226 227 229 230 232 233 234 236 237 238 239 240 241 243 244 245 247 250\n",
      " 251 252 253 254 255 256 257 258 260 261 263 264 265 267 268 269 270 272\n",
      " 273 274 276 277 278 279 280 281 282 284 285 286 287 288 289 291 292 293\n",
      " 294 295 296 297 298 299 300 301 302 304 307 310 311 312 313 314 316 318\n",
      " 319 323 324 325 326 327 328 329 331 332 334 335 337 338 340 341 343 344\n",
      " 345 347 349 350 351 353 354 357 360 365 366 368 370 371 372 374 378 380\n",
      " 381 382 383 384 386 388 390 391 392 393 394 395 396 397 399 401 403 405\n",
      " 406 408 409 411 412 414 415 422 423 424 427 428 429 430 431 432 434 435\n",
      " 436 438 439 440 444 445 446 447 448 451 452 453 454 455 462 463 464 468\n",
      " 470 471 473 474 476 478 479 480 483 484 489 490 493 494 501 502 503 505\n",
      " 508 510 511 513 515 523 524 526 527 530 536 537 540 544 548 551 552 554\n",
      " 560 563 566 570 575 583 585 586 589 590 591 593 596 600 607 608 611 615\n",
      " 623 629 636 637 645 648 659 661 672 677 678 685 701 703 713 727 740 742\n",
      " 745 746 747 757 766 769 771 779 814 825 844 889]\n",
      "f3: [ 64  71  77  83  86  88  94  95  97  99 101 104 107 110 112 113 114 115\n",
      " 117 118 121 123 126 127 128 130 131 133 136 137 140 142 143 145 146 147\n",
      " 150 151 152 154 157 159 160 161 163 165 166 167 169 171 172 173 174 175\n",
      " 176 177 178 179 180 186 187 188 189 190 192 193 194 195 196 197 199 200\n",
      " 201 202 203 205 206 209 210 211 214 216 217 219 220 222 223 224 225 226\n",
      " 227 229 230 232 233 234 236 237 238 239 240 241 243 244 245 247 250 251\n",
      " 252 253 254 255 256 257 258 260 261 263 264 265 267 268 269 270 272 273\n",
      " 274 276 277 278 279 280 281 282 284 285 286 287 288 289 291 292 293 294\n",
      " 295 296 297 298 299 300 301 302 304 307 310 311 312 313 314 316 318 319\n",
      " 323 324 325 326 327 328 329 331 332 334 335 337 338 340 341 343 344 345\n",
      " 347 349 350 351 353 354 357 360 365 366 368 370 371 372 374 378 380 381\n",
      " 382 383 384 386 388 390 391 392 393 394 395 396 397 399 401 403 405 406\n",
      " 408 409 411 412 414 415 422 423 424 427 428 429 430 431 432 434 435 436\n",
      " 438 439 440 444 445 446 447 448 451 452 453 454 455 462 463 464 468 470\n",
      " 471 473 474 476 478 479 480 483 484 489 490 493 494 501 502 503 505 508\n",
      " 510 511 513 515 523 524 526 527 530 536 537 540 544 548 551 552 554 560\n",
      " 563 566 570 575 583 585 586 589 590 591 593 596 600 607 608 611 615 623\n",
      " 629 636 637 645 648 659 661 672 677 678 685 701 703 713 727 740 742 745\n",
      " 746 747 757 766 769 771 779 814 825 844 889]\n",
      "f4: [ 63  71  76  82  85  87  88  90  93  94  97  99 103 106 107 109 110 111\n",
      " 114 117 120 121 124 125 126 127 128 131 133 134 136 137 139 140 141 142\n",
      " 143 144 146 148 150 154 155 156 157 158 160 161 166 167 168 169 171 172\n",
      " 173 174 175 176 177 178 179 181 184 186 187 188 189 190 191 193 194 195\n",
      " 196 197 198 200 201 202 205 206 207 208 209 210 213 214 215 216 217 219\n",
      " 220 221 222 223 224 225 226 227 229 230 232 233 234 235 236 237 238 239\n",
      " 240 241 243 244 247 248 249 251 252 253 254 255 256 257 258 259 265 267\n",
      " 268 270 271 272 273 274 277 278 279 280 281 282 283 285 286 287 288 289\n",
      " 290 292 293 294 295 296 297 298 299 301 304 305 307 309 312 313 314 315\n",
      " 316 317 318 320 322 323 324 325 326 328 329 330 333 334 335 337 338 339\n",
      " 340 342 343 344 345 347 348 349 350 352 353 354 356 359 360 364 365 367\n",
      " 368 369 370 371 373 374 375 378 380 381 382 383 386 388 390 391 392 393\n",
      " 394 395 399 400 403 405 406 408 409 412 413 417 418 420 421 423 424 426\n",
      " 427 429 430 433 436 438 439 442 443 444 447 448 449 450 453 454 455 462\n",
      " 467 468 472 473 474 477 478 479 481 483 486 487 488 497 498 500 501 505\n",
      " 508 509 511 513 522 523 526 530 535 536 537 540 543 550 551 552 557 559\n",
      " 566 568 570 572 581 584 585 588 594 596 597 599 601 607 608 613 614 616\n",
      " 622 629 634 637 647 657 661 671 675 677 681 695 700 706 712 713 723 734\n",
      " 741 742 744 753 762 764 768 779 814 823 843 886]\n"
     ]
    }
   ],
   "source": [
    "# check the length of the features\n",
    "for feature in ['lpc', 'log_energy_entropy', 'shannon_entropy', 'threshold_entropy',\n",
    "                   'sure_entropy', 'rms', 'f0', 'f1', 'f2', 'f3', 'f4']:\n",
    "    length_feat = df_paper1[feature].apply(lambda x: len(x))\n",
    "    print(f'{feature}: {np.sort(length_feat.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71453e4f",
   "metadata": {},
   "source": [
    "-> we have to pad each vector within a feature to the same length such that we can run a neural net on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5081160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lpc': 5, 'log_energy_entropy': 281, 'shannon_entropy': 281, 'threshold_entropy': 281, 'sure_entropy': 281, 'rms': 281, 'f0': 281, 'f1': 889, 'f2': 889, 'f3': 889, 'f4': 886}\n"
     ]
    }
   ],
   "source": [
    "# get maximal input length per feature\n",
    "length_dict = {}\n",
    "for feature in df_paper1.drop(columns=['label']).columns:\n",
    "    try:\n",
    "        length_dict[feature] = np.max(df_paper1[feature].apply(lambda x: len(x)))\n",
    "    except:\n",
    "        continue\n",
    "print(length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7332b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vector(vector, length):\n",
    "    \"\"\"\n",
    "    Pad a vector with zeros to a given length.\n",
    "\n",
    "    Args:\n",
    "        vector (list or numpy.ndarray or torch.Tensor): Input vector.\n",
    "        length (int): Desired length of padded vector.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded vector.\n",
    "    \"\"\"\n",
    "    if isinstance(vector, list):\n",
    "        vector = torch.Tensor(vector)\n",
    "    elif isinstance(vector, np.ndarray):\n",
    "        vector = torch.from_numpy(vector)\n",
    "    elif not isinstance(vector, torch.Tensor):\n",
    "        raise TypeError(\"Input vector must be a list, numpy.ndarray, or torch.Tensor.\")\n",
    "\n",
    "    if length <= vector.size(0):\n",
    "        return vector\n",
    "\n",
    "    pad_size = (0, length - vector.size(0))\n",
    "    return nn.functional.pad(vector, pad_size, mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "702fd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad each entry to the maximal feature length for consisten computing\n",
    "for feature in length_dict.keys():\n",
    "    df_paper1[feature] = df_paper1[feature].apply(lambda x: pad_vector(x, length_dict[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b4afe817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>var</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>lpc</th>\n",
       "      <th>log_energy_entropy</th>\n",
       "      <th>shannon_entropy</th>\n",
       "      <th>threshold_entropy</th>\n",
       "      <th>sure_entropy</th>\n",
       "      <th>rms</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happiness</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>-0.811890</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>[tensor(1.), tensor(-1.3784), tensor(0.6983), ...</td>\n",
       "      <td>[tensor(0.8510), tensor(0.3770), tensor(0.), t...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.240...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.534...</td>\n",
       "      <td>[tensor(0.0011), tensor(0.0013), tensor(0.0125...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(366.1395), tensor(406.2651), tensor(46...</td>\n",
       "      <td>[tensor(1888.5991), tensor(1888.0465), tensor(...</td>\n",
       "      <td>[tensor(2622.3481), tensor(2640.6831), tensor(...</td>\n",
       "      <td>[tensor(3650.5303), tensor(3616.1926), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>-0.999847</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>[tensor(1.), tensor(-1.5548), tensor(0.8446), ...</td>\n",
       "      <td>[tensor(0.7780), tensor(0.3398), tensor(0.0070...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.300...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.546...</td>\n",
       "      <td>[tensor(0.0023), tensor(0.0025), tensor(0.0158...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(455.1343), tensor(474.1310), tensor(51...</td>\n",
       "      <td>[tensor(1710.5642), tensor(1677.8259), tensor(...</td>\n",
       "      <td>[tensor(2032.8323), tensor(2660.1262), tensor(...</td>\n",
       "      <td>[tensor(2662.4736), tensor(3074.6057), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>[tensor(1.), tensor(-1.3665), tensor(0.8410), ...</td>\n",
       "      <td>[tensor(0.6903), tensor(0.3466), tensor(0.), t...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.149...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(0.1101, dtype=torch.float64), tensor(0...</td>\n",
       "      <td>[tensor(0.0004), tensor(0.0005), tensor(0.0006...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(412.2984), tensor(457.1609), tensor(49...</td>\n",
       "      <td>[tensor(1910.5836), tensor(1912.4899), tensor(...</td>\n",
       "      <td>[tensor(2687.4016), tensor(2687.0125), tensor(...</td>\n",
       "      <td>[tensor(3211.2520), tensor(3124.9492), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happiness</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>-0.924469</td>\n",
       "      <td>0.027802</td>\n",
       "      <td>[tensor(1.), tensor(-1.1567), tensor(0.0952), ...</td>\n",
       "      <td>[tensor(0.7458), tensor(0.3506), tensor(0.2159...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.247...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.532...</td>\n",
       "      <td>[tensor(0.0020), tensor(0.0049), tensor(0.1031...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(467.4830), tensor(528.4268), tensor(59...</td>\n",
       "      <td>[tensor(1645.6774), tensor(1663.4481), tensor(...</td>\n",
       "      <td>[tensor(2508.0247), tensor(2576.8511), tensor(...</td>\n",
       "      <td>[tensor(3766.8508), tensor(3893.7004), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>-0.999847</td>\n",
       "      <td>0.028595</td>\n",
       "      <td>[tensor(1.), tensor(-1.4481), tensor(0.6666), ...</td>\n",
       "      <td>[tensor(0.7931), tensor(0.4573), tensor(0.1627...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.366...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0.551...</td>\n",
       "      <td>[tensor(0.0043), tensor(0.0050), tensor(0.0118...</td>\n",
       "      <td>[tensor(0., dtype=torch.float64), tensor(0., d...</td>\n",
       "      <td>[tensor(422.4506), tensor(420.6553), tensor(40...</td>\n",
       "      <td>[tensor(1533.5787), tensor(1536.3580), tensor(...</td>\n",
       "      <td>[tensor(2599.1267), tensor(2500.3315), tensor(...</td>\n",
       "      <td>[tensor(3834.8613), tensor(3813.5173), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label       var       min    median  \\\n",
       "0  happiness  0.006776 -0.811890  0.017822   \n",
       "1    neutral  0.015701 -0.999847  0.019135   \n",
       "2      anger  0.016229 -0.999969  0.024933   \n",
       "3  happiness  0.015496 -0.924469  0.027802   \n",
       "4    neutral  0.024153 -0.999847  0.028595   \n",
       "\n",
       "                                                 lpc  \\\n",
       "0  [tensor(1.), tensor(-1.3784), tensor(0.6983), ...   \n",
       "1  [tensor(1.), tensor(-1.5548), tensor(0.8446), ...   \n",
       "2  [tensor(1.), tensor(-1.3665), tensor(0.8410), ...   \n",
       "3  [tensor(1.), tensor(-1.1567), tensor(0.0952), ...   \n",
       "4  [tensor(1.), tensor(-1.4481), tensor(0.6666), ...   \n",
       "\n",
       "                                  log_energy_entropy  \\\n",
       "0  [tensor(0.8510), tensor(0.3770), tensor(0.), t...   \n",
       "1  [tensor(0.7780), tensor(0.3398), tensor(0.0070...   \n",
       "2  [tensor(0.6903), tensor(0.3466), tensor(0.), t...   \n",
       "3  [tensor(0.7458), tensor(0.3506), tensor(0.2159...   \n",
       "4  [tensor(0.7931), tensor(0.4573), tensor(0.1627...   \n",
       "\n",
       "                                     shannon_entropy  \\\n",
       "0  [tensor(0., dtype=torch.float64), tensor(0.240...   \n",
       "1  [tensor(0., dtype=torch.float64), tensor(0.300...   \n",
       "2  [tensor(0., dtype=torch.float64), tensor(0.149...   \n",
       "3  [tensor(0., dtype=torch.float64), tensor(0.247...   \n",
       "4  [tensor(0., dtype=torch.float64), tensor(0.366...   \n",
       "\n",
       "                                   threshold_entropy  \\\n",
       "0  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "1  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "2  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "3  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "4  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "\n",
       "                                        sure_entropy  \\\n",
       "0  [tensor(0., dtype=torch.float64), tensor(0.534...   \n",
       "1  [tensor(0., dtype=torch.float64), tensor(0.546...   \n",
       "2  [tensor(0.1101, dtype=torch.float64), tensor(0...   \n",
       "3  [tensor(0., dtype=torch.float64), tensor(0.532...   \n",
       "4  [tensor(0., dtype=torch.float64), tensor(0.551...   \n",
       "\n",
       "                                                 rms  \\\n",
       "0  [tensor(0.0011), tensor(0.0013), tensor(0.0125...   \n",
       "1  [tensor(0.0023), tensor(0.0025), tensor(0.0158...   \n",
       "2  [tensor(0.0004), tensor(0.0005), tensor(0.0006...   \n",
       "3  [tensor(0.0020), tensor(0.0049), tensor(0.1031...   \n",
       "4  [tensor(0.0043), tensor(0.0050), tensor(0.0118...   \n",
       "\n",
       "                                                  f0  \\\n",
       "0  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "1  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "2  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "3  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "4  [tensor(0., dtype=torch.float64), tensor(0., d...   \n",
       "\n",
       "                                                  f1  \\\n",
       "0  [tensor(366.1395), tensor(406.2651), tensor(46...   \n",
       "1  [tensor(455.1343), tensor(474.1310), tensor(51...   \n",
       "2  [tensor(412.2984), tensor(457.1609), tensor(49...   \n",
       "3  [tensor(467.4830), tensor(528.4268), tensor(59...   \n",
       "4  [tensor(422.4506), tensor(420.6553), tensor(40...   \n",
       "\n",
       "                                                  f2  \\\n",
       "0  [tensor(1888.5991), tensor(1888.0465), tensor(...   \n",
       "1  [tensor(1710.5642), tensor(1677.8259), tensor(...   \n",
       "2  [tensor(1910.5836), tensor(1912.4899), tensor(...   \n",
       "3  [tensor(1645.6774), tensor(1663.4481), tensor(...   \n",
       "4  [tensor(1533.5787), tensor(1536.3580), tensor(...   \n",
       "\n",
       "                                                  f3  \\\n",
       "0  [tensor(2622.3481), tensor(2640.6831), tensor(...   \n",
       "1  [tensor(2032.8323), tensor(2660.1262), tensor(...   \n",
       "2  [tensor(2687.4016), tensor(2687.0125), tensor(...   \n",
       "3  [tensor(2508.0247), tensor(2576.8511), tensor(...   \n",
       "4  [tensor(2599.1267), tensor(2500.3315), tensor(...   \n",
       "\n",
       "                                                  f4  \n",
       "0  [tensor(3650.5303), tensor(3616.1926), tensor(...  \n",
       "1  [tensor(2662.4736), tensor(3074.6057), tensor(...  \n",
       "2  [tensor(3211.2520), tensor(3124.9492), tensor(...  \n",
       "3  [tensor(3766.8508), tensor(3893.7004), tensor(...  \n",
       "4  [tensor(3834.8613), tensor(3813.5173), tensor(...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paper1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "45dd142f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8510, 0.3770, 0.0000, 0.2376, 0.5617, 0.8522, 1.0000, 0.9855, 0.8797,\n",
       "        0.8522, 0.9374, 0.9850, 0.8760, 0.6776, 0.4143, 0.2000, 0.2409, 0.4619,\n",
       "        0.6554, 0.7925, 0.8368, 0.8290, 0.8377, 0.7419, 0.5803, 0.4354, 0.3988,\n",
       "        0.5571, 0.6826, 0.7003, 0.5049, 0.4296, 0.5401, 0.6508, 0.7822, 0.7011,\n",
       "        0.6530, 0.6673, 0.7799, 0.9264, 0.8961, 0.8110, 0.7376, 0.7014, 0.7337,\n",
       "        0.7042, 0.5729, 0.5387, 0.5397, 0.6266, 0.7733, 0.7772, 0.7115, 0.5781,\n",
       "        0.4514, 0.2775, 0.1490, 0.0474, 0.2874, 0.7198, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of how the padded tensor looks like\n",
    "df_paper1['log_energy_entropy'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b30e2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train/test/val data\n",
    "X_train, X_test, y_train, y_test = load_train_test_data(df_paper1, verbose=False)\n",
    "df_test = pd.concat([X_test, y_test], axis = 1)\n",
    "X_val, X_test, y_val, y_test = load_train_test_data(df_test, test_size=0.5, split_type='val/test', verbose=False)\n",
    "num_classes = y_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3fa2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train, test, and val loader\n",
    "torch.manual_seed(0) # set random seed\n",
    "train_loader = DataLoader(TabularDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TabularDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(TabularDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "66c6612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1 = nn.Linear(sum(input_dims), hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate input features along the second axis\n",
    "        x = [torch.Tensor(f) for f in x]\n",
    "        x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0.0)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5bbdd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_lengths(df):\n",
    "    \"\"\"\n",
    "    Determine the length of each feature in a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        list: List of feature lengths.\n",
    "    \"\"\"\n",
    "    feature_lengths = []\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            feature_lengths.append(1)\n",
    "        elif pd.api.types.is_object_dtype(df[column]):\n",
    "            unique_values = df[column].unique()\n",
    "            feature_lengths.append(len(max(unique_values, key=len)))\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type for column {column}.\")\n",
    "    return feature_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf7ccd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 5, 281, 281, 281, 281, 281, 281, 889, 889, 889, 886]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the input dimensions\n",
    "INPUT_DIMS = get_feature_lengths(X_train)\n",
    "INPUT_DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "768f1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, optimizer, and criterion\n",
    "HIDDEN_SIZE = int(len(X_train.columns) / 2)\n",
    "model = MLP(input_dims=INPUT_DIMS, hidden_dim=HIDDEN_SIZE, output_dim=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d504c0be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, train_losses, val_losses, val_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, device, batch_size, num_epochs, patience)\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     16\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     17\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\automatic_speech_emotion_recognition\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\automatic_speech_emotion_recognition\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\automatic_speech_emotion_recognition\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\automatic_speech_emotion_recognition\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[31], line 11\u001b[0m, in \u001b[0;36mTabularDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39miloc[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses, val_f1_scores = train_model(model, criterion, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4508e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
